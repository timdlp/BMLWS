{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Tutorial_VI_tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PCKy-0EotsrX",
        "y7-p8ClctsrY"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbpit0_rtspv"
      },
      "source": [
        "# Tutorial VI: Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Fr8e3Ltspx"
      },
      "source": [
        "<p>\n",
        "Bern Winter School on Machine Learning, 2021<br>\n",
        "Prepared by Mykhailo Vladymyrov.\n",
        "</p>\n",
        "\n",
        "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Upuaj8tspy"
      },
      "source": [
        "In this session we will see what RNN is. We will use it to predict/generate text sequence, but same approach can be applied to any sequential data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QUO3V3Stspz"
      },
      "source": [
        "So far we looked at the data available altogether. In many cases the data is sequential (weather, speach, sensor signals etc).\n",
        "RNNs are specifically designed for such tasks.\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/BMLWS/raw/main/figures/rnn.png\" alt=\"drawing\" width=\"90%\"/><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGyY2JPXtsq7"
      },
      "source": [
        "## 1. Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkz4sJrWTQRi"
      },
      "source": [
        "colab = True # set to True is using google colab"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jKCF9MpKWdF"
      },
      "source": [
        "if colab:\n",
        "    %tensorflow_version 2.x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpRETZFNtsq7",
        "outputId": "6feecea4-45d5-4407-b838-6fcf75057717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipyd\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import time\n",
        "\n",
        "# We'll tell matplotlib to inline any drawn figures like so:\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"\"\"<style> .rendered_html code { \n",
        "    padding: 2px 5px;\n",
        "    color: #0000aa;\n",
        "    background-color: #cccccc;\n",
        "} </style>\"\"\")\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH0SSbAftsq1"
      },
      "source": [
        "## unpack libraries\n",
        "if using colab, run the next cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grv04xmitsq2",
        "outputId": "ed62069c-ee43-4ff6-f15f-7820e6cb3756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if colab:\n",
        "    p = tf.keras.utils.get_file('./material.tgz', 'https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz')\n",
        "    !mv {p} .\n",
        "    !tar -xvzf material.tgz > /dev/null 2>&1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/neworldemancer/BMLWS/raw/main/tut_files/tpub0320.tgz\n",
            "81215488/81207979 [==============================] - 1s 0us/step\n",
            "81223680/81207979 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWt72gnKj4M"
      },
      "source": [
        "from utils import gr_disp"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF9vrtHFTQR9"
      },
      "source": [
        "def show_graph(g=None, gd=None):\n",
        "    gr_disp.show_graph_eager(g, gd)\n",
        "    %tensorboard --logdir logs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7FnR5Kwtsq9"
      },
      "source": [
        "## 2. Load the text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3cmvKeatsq-"
      },
      "source": [
        "def read_data(fname):\n",
        "    with open(fname) as f:\n",
        "        content = f.readlines()\n",
        "    content = [x.strip() for x in content]\n",
        "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
        "    content = np.array(content)\n",
        "    return content"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4KJQxKqtsrA"
      },
      "source": [
        "training_file = 'RNN/rnn.txt'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMumjsH8tsrD"
      },
      "source": [
        "training_data = read_data(training_file)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_GMc64ptsrF",
        "outputId": "406235ce-5454-46c4-b778-b2746cdbd81a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(training_data[:100])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['recurrent' 'neural' 'networks' ',' 'or' 'rnns' '(' 'rumelhart' 'et' 'al'\n",
            " '.' ',' '1986a' ')' ',' 'are' 'a' 'family' 'of' 'neural' 'networks' 'for'\n",
            " 'processing' 'sequential' 'data' '.' 'much' 'as' 'a' 'convolutional'\n",
            " 'network' 'is' 'a' 'neural' 'network' 'that' 'is' 'specialized' 'for'\n",
            " 'processing' 'a' 'grid' 'of' 'values' 'such' 'as' 'an' 'image' ',' 'a'\n",
            " 'recurrent' 'neural' 'network' 'is' 'a' 'neural' 'network' 'that' 'is'\n",
            " 'specialized' 'for' 'processing' 'a' 'sequence' 'of' 'values' 'x' '(' '1'\n",
            " ')' ',' '.' '.' '.' ',' 'x' '(' 'tau' ')' '.' 'just' 'as' 'convolutional'\n",
            " 'networks' 'can' 'readily' 'scale' 'to' 'images' 'with' 'large' 'width'\n",
            " 'and' 'height' ',' 'and' 'some' 'convolutional' 'networks' 'can']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvQjWvbWtsrH"
      },
      "source": [
        "## 3. Build dataset\n",
        "We will assign an id to each word, and make dictionaries word->id and id->word.\n",
        "The most frequently repeating words have lowest id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqWfeze4tsrI"
      },
      "source": [
        "def build_dataset(words):\n",
        "    count = collections.Counter(words).most_common()\n",
        "    dictionary = {}\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return dictionary, reverse_dictionary"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxjTb5VUtsrK"
      },
      "source": [
        "dictionary, reverse_dictionary = build_dataset(training_data)\n",
        "vocab_size = len(dictionary)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJt3d4lJtsrL",
        "outputId": "ca7e523e-ae79-40b1-c9fd-5dd659af71c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(dictionary)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 0, '.': 1, ',': 2, 'of': 3, '(': 4, ')': 5, 'a': 6, 'to': 7, 'in': 8, 't': 9, 'is': 10, 'time': 11, '1': 12, 'sequence': 13, 'x': 14, 'that': 15, 'for': 16, 'as': 17, 'recurrent': 18, 'we': 19, 'network': 20, '10': 21, 'networks': 22, '-': 23, 'and': 24, 'neural': 25, 'can': 26, 'state': 27, 'with': 28, 'be': 29, 'each': 30, 'graph': 31, '2': 32, 'different': 33, 'this': 34, ';': 35, 'it': 36, 'at': 37, 'computational': 38, 'f': 39, 'such': 40, 'parameters': 41, 'across': 42, 'model': 43, 'equation': 44, 'h': 45, 'length': 46, 'function': 47, 'step': 48, 's': 49, '=': 50, 'an': 51, 'variable': 52, 'sharing': 53, 'information': 54, 'same': 55, 'on': 56, 'theta': 57, 'tau': 58, 'than': 59, 'one': 60, 'not': 61, 'input': 62, 'by': 63, 'steps': 64, 'use': 65, 'output': 66, 'unfolded': 67, 'would': 68, 'from': 69, 'lengths': 70, 'example': 71, 'two': 72, 'all': 73, 'way': 74, 'unfolding': 75, 'figure': 76, 'past': 77, 'g': 78, 'or': 79, 'rnns': 80, 'convolutional': 81, ':': 82, 'possible': 83, 'if': 84, 'separate': 85, 'training': 86, 'when': 87, '\"': 88, 'sentence': 89, 'idea': 90, 'convolution': 91, 'member': 92, 'refer': 93, 'chapter': 94, 'many': 95, 'structure': 96, '5': 97, 'has': 98, 'definition': 99, '3': 100, 'now': 101, 'et': 102, 'al': 103, 'processing': 104, 'data': 105, 'much': 106, 'values': 107, 'images': 108, 'process': 109, 'size': 110, 'sequences': 111, 'need': 112, 'learning': 113, 'statistical': 114, 'parameter': 115, 'value': 116, 'index': 117, 'share': 118, 'consider': 119, 'went': 120, 'nepal': 121, '2009': 122, 'word': 123, 'have': 124, 'learn': 125, 'allows': 126, 'where': 127, 'previous': 128, 'applied': 129, 'may': 130, 'represent': 131, 'future': 132, 'us': 133, 'ways': 134, 'typically': 135, 'system': 136, 'recurrence': 137, 'summary': 138, 'aspects': 139, 'rnn': 140, 'component': 141, 'circuit': 142, 'single': 143, 'specialized': 144, 'scale': 145, 'some': 146, 'without': 147, 'most': 148, 'also': 149, 'go': 150, 'machine': 151, 'parts': 152, 'examples': 153, 'generalize': 154, 'positions': 155, 'piece': 156, 'sentences': 157, 'i': 158, 'ask': 159, 'read': 160, 'year': 161, 'which': 162, 'relevant': 163, 'trained': 164, 'feedforward': 165, 'fixed': 166, 'traditional': 167, 'language': 168, 'position': 169, 'delay': 170, 'lang': 171, 'operation': 172, 'but': 173, 'number': 174, 'members': 175, 'application': 176, 'using': 177, 'outputs': 178, 'results': 179, 'deep': 180, 'contains': 181, 'minibatch': 182, 'real': 183, 'refers': 184, 'only': 185, 'involving': 186, 'provided': 187, 'cycles': 188, 'these': 189, 'influence': 190, 'allow': 191, 'define': 192, 'more': 193, 'set': 194, 'inputs': 195, 'section': 196, '6': 197, 'general': 198, 'into': 199, 'dynamical': 200, 'because': 201, 'applying': 202, 'expression': 203, 'represented': 204, 'illustrated': 205, 'another': 206, '4': 207, 'whole': 208, 'any': 209, 'considered': 210, 'their': 211, 'hidden': 212, 'units': 213, 'indicate': 214, 'make': 215, 'task': 216, 'lossy': 217, 'up': 218, 'maps': 219, 'might': 220, 'other': 221, 'predict': 222, 'storing': 223, 'enough': 224, 'drawn': 225, 'draw': 226, 'diagram': 227, 'node': 228, 'every': 229, 'physical': 230, 'operates': 231, 'current': 232, 'left': 233, 'takes': 234, 'right': 235, 'side': 236, 'repeated': 237, 'specified': 238, 'terms': 239, 'transition': 240, 'rather': 241, 'rumelhart': 242, '1986a': 243, 'are': 244, 'family': 245, 'sequential': 246, 'grid': 247, 'image': 248, 'just': 249, 'readily': 250, 'large': 251, 'width': 252, 'height': 253, 'longer': 254, 'practical': 255, 'based': 256, 'specialization': 257, 'multilayer': 258, 'take': 259, 'advantage': 260, 'early': 261, 'ideas': 262, 'found': 263, 'models': 264, 'the1980s': 265, 'makes': 266, 'extend': 267, 'apply': 268, 'forms': 269, 'here': 270, 'them': 271, 'had': 272, 'could': 273, 'seen': 274, 'during': 275, 'nor': 276, 'strength': 277, 'particularly': 278, 'important': 279, 'specific': 280, 'occur': 281, 'multiple': 282, 'within': 283, 'extract': 284, 'narrator': 285, 'like': 286, 'recognize': 287, 'whether': 288, 'appears': 289, 'sixth': 290, 'second': 291, 'suppose': 292, 'processes': 293, 'fully': 294, 'connected': 295, 'feature': 296, 'so': 297, 'rules': 298, 'separately': 299, 'comparison': 300, 'shares': 301, 'weights': 302, 'several': 303, 'related': 304, 'd': 305, 'temporal': 306, 'approach': 307, 'basis': 308, 'andhinton': 309, '1988': 310, 'waibel': 311, '1989': 312, '1990': 313, 'shallow': 314, 'small': 315, 'neighboring': 316, 'manifests': 317, 'kernel': 318, 'produced': 319, 'update': 320, 'rule': 321, 'formulation': 322, 'through': 323, 'very': 324, 'simplicity': 325, 'exposition': 326, 'operating': 327, 'vectors': 328, 'ranging': 329, 'practice': 330, 'usually': 331, 'operate': 332, 'minibatches': 333, 'omitted': 334, 'indices': 335, 'simplify': 336, 'notation': 337, 'moreover': 338, 'literally': 339, 'passage': 340, 'world': 341, 'sometimes': 342, 'dimensions': 343, 'spatial': 344, 'even': 345, 'connections': 346, 'backward': 347, 'entire': 348, 'observed': 349, 'before': 350, 'extends': 351, 'include': 352, 'present': 353, 'its': 354, 'own': 355, 'graphs': 356, 'then': 357, 'describe': 358, 'construct': 359, 'train': 360, 'available': 361, 'reader': 362, 'textbook': 363, 'graves': 364, '2012': 365, 'formalize': 366, 'computations': 367, 'those': 368, 'involved': 369, 'mapping': 370, 'loss': 371, 'please': 372, 'introduction': 373, 'explain': 374, 'recursive': 375, 'computation': 376, 'repetitive': 377, 'corresponding': 378, 'chain': 379, 'events': 380, 'classical': 381, 'form': 382, 'called': 383, 'back': 384, 'finite': 385, 'times': 386, 'unfold': 387, 'obtains': 388, 'repeatedly': 389, 'yielded': 390, 'does': 391, 'involve': 392, 'directed': 393, 'acyclic': 394, 'let': 395, 'driven': 396, 'external': 397, 'signal': 398, 'see': 399, 'about': 400, 'built': 401, 'almost': 402, 'feed': 403, 'forward': 404, 'essentially': 405, 'similar': 406, 'rewrite': 407, 'typical': 408, 'will': 409, 'add': 410, 'extra': 411, 'architectural': 412, 'features': 413, 'layers': 414, 'out': 415, 'predictions': 416, 'perform': 417, 'requires': 418, 'predicting': 419, 'learns': 420, 'kind': 421, 'tot': 422, 'necessarily': 423, 'since': 424, 'arbitrary': 425, 'vector': 426, 'depending': 427, 'criterion': 428, 'selectively': 429, 'keep': 430, 'precision': 431, 'used': 432, 'modeling': 433, 'next': 434, 'given': 435, 'words': 436, 'necessary': 437, 'rest': 438, 'sufficient': 439, 'demanding': 440, 'situation': 441, 'rich': 442, 'approximately': 443, 'recover': 444, 'autoencoder': 445, 'frameworks': 446, '14': 447, 'containing': 448, 'exist': 449, 'implementation': 450, 'biological': 451, 'view': 452, 'defines': 453, 'whose': 454, 'throughout': 455, 'black': 456, 'square': 457, 'interaction': 458, 'place': 459, '+': 460, 'variables': 461, 'per': 462, 'representing': 463, 'point': 464, 'what': 465, 'call': 466, 'pieces': 467, 'depends': 468, 'after': 469, '7': 470, 'produces': 471, 'factorize': 472, 'thus': 473, 'introduces': 474, 'major': 475, 'advantages': 476, 'regardless': 477, 'learned': 478, 'always': 479, 'history': 480, 'states': 481, 'factors': 482, 'needing': 483, 'shared': 484, 'generalization': 485, 'did': 486, 'appear': 487, 'enables': 488, 'estimated': 489, 'far': 490, 'fewer': 491, 'required': 492}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1jtdvAztsrN"
      },
      "source": [
        "Then the whole text will look as a sequence of word ids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNVe-0P_tsrO",
        "outputId": "f9b09e40-1322-43e0-f8f7-97db5757eaab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words_as_int = [dictionary[w] for w in training_data]\n",
        "print(words_as_int)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 25, 22, 2, 79, 80, 4, 242, 102, 103, 1, 2, 243, 5, 2, 244, 6, 245, 3, 25, 22, 16, 104, 246, 105, 1, 106, 17, 6, 81, 20, 10, 6, 25, 20, 15, 10, 144, 16, 104, 6, 247, 3, 107, 40, 17, 51, 248, 2, 6, 18, 25, 20, 10, 6, 25, 20, 15, 10, 144, 16, 104, 6, 13, 3, 107, 14, 4, 12, 5, 2, 1, 1, 1, 2, 14, 4, 58, 5, 1, 249, 17, 81, 22, 26, 250, 145, 7, 108, 28, 251, 252, 24, 253, 2, 24, 146, 81, 22, 26, 109, 108, 3, 52, 110, 2, 18, 22, 26, 145, 7, 106, 254, 111, 59, 68, 29, 255, 16, 22, 147, 13, 23, 256, 257, 1, 148, 18, 22, 26, 149, 109, 111, 3, 52, 46, 1, 7, 150, 69, 258, 22, 7, 18, 22, 2, 19, 112, 7, 259, 260, 3, 60, 3, 0, 261, 262, 263, 8, 151, 113, 24, 114, 264, 3, 265, 82, 53, 41, 42, 33, 152, 3, 6, 43, 1, 115, 53, 266, 36, 83, 7, 267, 24, 268, 0, 43, 7, 153, 3, 33, 269, 4, 33, 70, 2, 270, 5, 24, 154, 42, 271, 1, 84, 19, 272, 85, 41, 16, 30, 116, 3, 0, 11, 117, 2, 19, 273, 61, 154, 7, 13, 70, 61, 274, 275, 86, 2, 276, 118, 114, 277, 42, 33, 13, 70, 24, 42, 33, 155, 8, 11, 1, 40, 53, 10, 278, 279, 87, 6, 280, 156, 3, 54, 26, 281, 37, 282, 155, 283, 0, 13, 1, 16, 71, 2, 119, 0, 72, 157, 88, 158, 120, 7, 121, 8, 122, 88, 24, 88, 8, 122, 2, 158, 120, 7, 121, 1, 88, 84, 19, 159, 6, 151, 113, 43, 7, 160, 30, 89, 24, 284, 0, 161, 8, 162, 0, 285, 120, 7, 121, 2, 19, 68, 286, 36, 7, 287, 0, 161, 122, 17, 0, 163, 156, 3, 54, 2, 288, 36, 289, 8, 0, 290, 123, 79, 8, 0, 291, 123, 3, 0, 89, 1, 292, 15, 19, 164, 6, 165, 20, 15, 293, 157, 3, 166, 46, 1, 6, 167, 294, 295, 165, 20, 68, 124, 85, 41, 16, 30, 62, 296, 2, 297, 36, 68, 112, 7, 125, 73, 0, 298, 3, 0, 168, 299, 37, 30, 169, 8, 0, 89, 1, 63, 300, 2, 6, 18, 25, 20, 301, 0, 55, 302, 42, 303, 11, 64, 1, 6, 304, 90, 10, 0, 65, 3, 91, 42, 6, 12, 23, 305, 306, 13, 1, 34, 81, 307, 10, 0, 308, 16, 11, 23, 170, 25, 22, 4, 171, 309, 2, 310, 35, 311, 102, 103, 1, 2, 312, 35, 171, 102, 103, 1, 2, 313, 5, 1, 0, 91, 172, 126, 6, 20, 7, 118, 41, 42, 11, 173, 10, 314, 1, 0, 66, 3, 91, 10, 6, 13, 127, 30, 92, 3, 0, 66, 10, 6, 47, 3, 6, 315, 174, 3, 316, 175, 3, 0, 62, 1, 0, 90, 3, 115, 53, 317, 8, 0, 176, 3, 0, 55, 91, 318, 37, 30, 11, 48, 1, 18, 22, 118, 41, 8, 6, 33, 74, 1, 30, 92, 3, 0, 66, 10, 6, 47, 3, 0, 128, 175, 3, 0, 66, 1, 30, 92, 3, 0, 66, 10, 319, 177, 0, 55, 320, 321, 129, 7, 0, 128, 178, 1, 34, 18, 322, 179, 8, 0, 53, 3, 41, 323, 6, 324, 180, 38, 31, 1, 16, 0, 325, 3, 326, 2, 19, 93, 7, 80, 17, 327, 56, 6, 13, 15, 181, 328, 14, 4, 9, 5, 28, 0, 11, 48, 117, 9, 329, 69, 12, 7, 58, 1, 8, 330, 2, 18, 22, 331, 332, 56, 333, 3, 40, 111, 2, 28, 6, 33, 13, 46, 58, 16, 30, 92, 3, 0, 182, 1, 19, 124, 334, 0, 182, 335, 7, 336, 337, 1, 338, 2, 0, 11, 48, 117, 112, 61, 339, 93, 7, 0, 340, 3, 11, 8, 0, 183, 341, 1, 342, 36, 184, 185, 7, 0, 169, 8, 0, 13, 1, 80, 130, 149, 29, 129, 8, 72, 343, 42, 344, 105, 40, 17, 108, 2, 24, 345, 87, 129, 7, 105, 186, 11, 2, 0, 20, 130, 124, 346, 15, 150, 347, 8, 11, 2, 187, 15, 0, 348, 13, 10, 349, 350, 36, 10, 187, 7, 0, 20, 1, 34, 94, 351, 0, 90, 3, 6, 38, 31, 7, 352, 188, 1, 189, 188, 131, 0, 190, 3, 0, 353, 116, 3, 6, 52, 56, 354, 355, 116, 37, 6, 132, 11, 48, 1, 40, 38, 356, 191, 133, 7, 192, 18, 25, 22, 1, 19, 357, 358, 95, 33, 134, 7, 359, 2, 360, 2, 24, 65, 18, 25, 22, 1, 16, 193, 54, 56, 18, 25, 22, 59, 10, 361, 8, 34, 94, 2, 19, 93, 0, 362, 7, 0, 363, 3, 364, 4, 365, 5, 1, 6, 38, 31, 10, 6, 74, 7, 366, 0, 96, 3, 6, 194, 3, 367, 2, 40, 17, 368, 369, 8, 370, 195, 24, 41, 7, 178, 24, 371, 1, 372, 93, 7, 196, 197, 1, 97, 1, 12, 16, 6, 198, 373, 1, 8, 34, 196, 19, 374, 0, 90, 3, 75, 6, 375, 79, 18, 376, 199, 6, 38, 31, 15, 98, 6, 377, 96, 2, 135, 378, 7, 6, 379, 3, 380, 1, 75, 34, 31, 179, 8, 0, 53, 3, 41, 42, 6, 180, 20, 96, 1, 16, 71, 2, 119, 0, 381, 382, 3, 6, 200, 136, 82, 49, 4, 9, 5, 50, 39, 4, 49, 4, 9, 23, 12, 5, 35, 57, 5, 2, 4, 21, 1, 12, 5, 127, 49, 4, 9, 5, 10, 383, 0, 27, 3, 0, 136, 1, 44, 21, 1, 12, 10, 18, 201, 0, 99, 3, 49, 37, 11, 9, 184, 384, 7, 0, 55, 99, 37, 11, 9, 23, 12, 1, 16, 6, 385, 174, 3, 11, 64, 58, 2, 0, 31, 26, 29, 67, 63, 202, 0, 99, 58, 23, 12, 386, 1, 16, 71, 2, 84, 19, 387, 44, 21, 1, 12, 16, 58, 50, 100, 11, 64, 2, 19, 388, 4, 100, 5, 50, 39, 4, 49, 4, 32, 5, 35, 57, 5, 4, 21, 1, 32, 5, 50, 39, 4, 39, 4, 49, 4, 12, 5, 35, 57, 5, 35, 57, 5, 1, 4, 21, 1, 100, 5, 75, 0, 44, 63, 389, 202, 0, 99, 8, 34, 74, 98, 390, 51, 203, 15, 391, 61, 392, 137, 1, 40, 51, 203, 26, 101, 29, 204, 63, 6, 167, 393, 394, 38, 31, 1, 0, 67, 38, 31, 3, 44, 21, 1, 12, 24, 44, 21, 1, 100, 10, 205, 8, 76, 21, 1, 12, 1, 17, 206, 71, 2, 395, 133, 119, 6, 200, 136, 396, 63, 51, 397, 398, 14, 4, 9, 5, 2, 49, 4, 9, 5, 50, 39, 4, 49, 4, 9, 23, 12, 5, 2, 14, 4, 9, 5, 35, 57, 5, 2, 4, 21, 1, 207, 5, 127, 19, 399, 15, 0, 27, 101, 181, 54, 400, 0, 208, 77, 13, 1, 18, 25, 22, 26, 29, 401, 8, 95, 33, 134, 1, 106, 17, 402, 209, 47, 26, 29, 210, 6, 403, 404, 25, 20, 2, 405, 209, 47, 186, 137, 26, 29, 210, 6, 18, 25, 20, 1, 95, 18, 25, 22, 65, 44, 21, 1, 97, 79, 6, 406, 44, 7, 192, 0, 107, 3, 211, 212, 213, 1, 7, 214, 15, 0, 27, 10, 0, 212, 213, 3, 0, 20, 2, 19, 101, 407, 44, 21, 1, 207, 177, 0, 52, 45, 7, 131, 0, 27, 2, 45, 4, 9, 5, 50, 39, 4, 45, 4, 9, 23, 12, 5, 2, 14, 4, 9, 5, 35, 57, 5, 2, 4, 21, 1, 97, 5, 205, 8, 76, 21, 1, 32, 35, 408, 80, 409, 410, 411, 412, 413, 40, 17, 66, 414, 15, 160, 54, 415, 3, 0, 27, 45, 7, 215, 416, 1, 87, 0, 18, 20, 10, 164, 7, 417, 6, 216, 15, 418, 419, 0, 132, 69, 0, 77, 2, 0, 20, 135, 420, 7, 65, 45, 4, 9, 5, 17, 6, 421, 3, 217, 138, 3, 0, 216, 23, 163, 139, 3, 0, 77, 13, 3, 195, 218, 422, 1, 34, 138, 10, 8, 198, 423, 217, 2, 424, 36, 219, 51, 425, 46, 13, 4, 14, 4, 9, 5, 2, 14, 4, 9, 23, 12, 5, 2, 14, 4, 9, 23, 32, 5, 2, 1, 1, 1, 2, 14, 4, 32, 5, 2, 14, 4, 12, 5, 5, 7, 6, 166, 46, 426, 45, 4, 9, 5, 1, 427, 56, 0, 86, 428, 2, 34, 138, 220, 429, 430, 146, 139, 3, 0, 77, 13, 28, 193, 431, 59, 221, 139, 1, 16, 71, 2, 84, 0, 140, 10, 432, 8, 114, 168, 433, 2, 135, 7, 222, 0, 434, 123, 435, 128, 436, 2, 223, 73, 0, 54, 8, 0, 62, 13, 218, 7, 11, 9, 130, 61, 29, 437, 35, 223, 185, 224, 54, 7, 222, 0, 438, 3, 0, 89, 10, 439, 1, 0, 148, 440, 441, 10, 87, 19, 159, 45, 4, 9, 5, 7, 29, 442, 224, 7, 191, 60, 7, 443, 444, 0, 62, 13, 2, 17, 8, 445, 446, 4, 94, 447, 5, 1, 44, 21, 1, 97, 26, 29, 225, 8, 72, 33, 134, 1, 60, 74, 7, 226, 0, 140, 10, 28, 6, 227, 448, 60, 228, 16, 229, 141, 15, 220, 449, 8, 6, 230, 450, 3, 0, 43, 2, 40, 17, 6, 451, 25, 20, 1, 8, 34, 452, 2, 0, 20, 453, 6, 142, 15, 231, 8, 183, 11, 2, 28, 230, 152, 454, 232, 27, 26, 190, 211, 132, 27, 2, 17, 8, 0, 233, 3, 76, 21, 1, 32, 1, 455, 34, 94, 2, 19, 65, 6, 456, 457, 8, 6, 142, 227, 7, 214, 15, 51, 458, 234, 459, 28, 6, 170, 3, 6, 143, 11, 48, 2, 69, 0, 27, 37, 11, 9, 7, 0, 27, 37, 11, 9, 460, 12, 1, 0, 221, 74, 7, 226, 0, 140, 10, 17, 51, 67, 38, 31, 2, 8, 162, 30, 141, 10, 204, 63, 95, 33, 461, 2, 28, 60, 52, 462, 11, 48, 2, 463, 0, 27, 3, 0, 141, 37, 15, 464, 8, 11, 1, 30, 52, 16, 30, 11, 48, 10, 225, 17, 6, 85, 228, 3, 0, 38, 31, 2, 17, 8, 0, 235, 3, 76, 21, 1, 32, 1, 465, 19, 466, 75, 10, 0, 172, 15, 219, 6, 142, 2, 17, 8, 0, 233, 236, 3, 0, 76, 2, 7, 6, 38, 31, 28, 237, 467, 2, 17, 8, 0, 235, 236, 1, 0, 67, 31, 101, 98, 6, 110, 15, 468, 56, 0, 13, 46, 1, 19, 26, 131, 0, 67, 137, 469, 9, 64, 28, 6, 47, 78, 4, 9, 5, 82, 45, 4, 9, 5, 50, 78, 4, 9, 5, 4, 14, 4, 9, 5, 2, 14, 4, 9, 23, 12, 5, 2, 14, 4, 9, 23, 32, 5, 2, 1, 1, 1, 2, 14, 4, 32, 5, 2, 14, 4, 12, 5, 5, 4, 21, 1, 197, 5, 50, 39, 4, 45, 4, 9, 23, 12, 5, 2, 14, 4, 9, 5, 35, 57, 5, 1, 4, 21, 1, 470, 5, 0, 47, 78, 4, 9, 5, 234, 0, 208, 77, 13, 4, 14, 4, 9, 5, 2, 14, 4, 9, 23, 12, 5, 2, 14, 4, 9, 23, 32, 5, 2, 1, 1, 1, 2, 14, 4, 32, 5, 2, 14, 4, 12, 5, 5, 17, 62, 24, 471, 0, 232, 27, 2, 173, 0, 67, 18, 96, 126, 133, 7, 472, 78, 4, 9, 5, 199, 237, 176, 3, 6, 47, 39, 1, 0, 75, 109, 473, 474, 72, 475, 476, 82, 12, 1, 477, 3, 0, 13, 46, 2, 0, 478, 43, 479, 98, 0, 55, 62, 110, 2, 201, 36, 10, 238, 8, 239, 3, 240, 69, 60, 27, 7, 206, 27, 2, 241, 59, 238, 8, 239, 3, 6, 52, 23, 46, 480, 3, 481, 1, 32, 1, 36, 10, 83, 7, 65, 0, 55, 240, 47, 39, 28, 0, 55, 41, 37, 229, 11, 48, 1, 189, 72, 482, 215, 36, 83, 7, 125, 6, 143, 43, 39, 15, 231, 56, 73, 11, 64, 24, 73, 13, 70, 2, 241, 59, 483, 7, 125, 6, 85, 43, 78, 4, 9, 5, 16, 73, 83, 11, 64, 1, 113, 6, 143, 484, 43, 126, 485, 7, 13, 70, 15, 486, 61, 487, 8, 0, 86, 194, 2, 24, 488, 0, 43, 7, 29, 489, 28, 490, 491, 86, 153, 59, 68, 29, 492, 147, 115, 53, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHSCevnwtsrQ"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phW3ru-dUSyv"
      },
      "source": [
        "We will build the model in TF2.\n",
        "It will contain an embedding layer, and three LSTM layers.\n",
        "Dense layer on top is used to output probability of the next word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Y8GDDqjvyx",
        "outputId": "c1cbfa42-a4fb-40dd-b7b6-dfece37e4202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Parameters\n",
        "n_input = 3  # word sequence to predict the following word\n",
        "\n",
        "# number of units in RNN cells\n",
        "n_hidden = [256, 512, 128]\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 128, input_length=n_input))\n",
        "\n",
        "for layer_i, n_h in enumerate(n_hidden):\n",
        "  model.add(tf.keras.layers.LSTM(n_h, return_sequences=True, name=f'{layer_i}_lstm{n_h}'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='RMSProp',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "W0 = model.get_weights()  # to reset model to original state:\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 128)            63104     \n",
            "                                                                 \n",
            " 0_lstm256 (LSTM)            (None, 3, 256)            394240    \n",
            "                                                                 \n",
            " 1_lstm512 (LSTM)            (None, 3, 512)            1574912   \n",
            "                                                                 \n",
            " 2_lstm128 (LSTM)            (None, 3, 128)            328192    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3, 493)            63597     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,424,045\n",
            "Trainable params: 2,424,045\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ojvV71pHJ6"
      },
      "source": [
        "## 5. Data streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_EYsL_TVaBz"
      },
      "source": [
        "Here we will see how to feed a dataset for model training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muEoZW05nFQt"
      },
      "source": [
        "# create tf.data.Dataset object\n",
        "word_dataset = tf.data.Dataset.from_tensor_slices(words_as_int)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfSyUC6CnlZ8",
        "outputId": "959ff44e-2358-4fa3-9ad0-85f61a3e7cd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# take metod generates elements:\n",
        "for i in word_dataset.take(5):\n",
        "  print(reverse_dictionary[i.numpy()])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recurrent\n",
            "neural\n",
            "networks\n",
            ",\n",
            "or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBK7FWPFVwSP"
      },
      "source": [
        "The `batch` method creates dataset, that generates sequences of elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtGgHXkPnc7P"
      },
      "source": [
        "sequences = word_dataset.batch(n_input+1, drop_remainder=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOcJ_Wgn3n_"
      },
      "source": [
        "# helper for int-to-text conversion\n",
        "to_text = lambda arr:' '.join([reverse_dictionary[it] for it in arr])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0DYh6wNnuJr",
        "outputId": "98e9f925-1a34-4a10-e100-b3a8554100be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for item in sequences.take(5):\n",
        "  print(to_text(item.numpy()))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recurrent neural networks ,\n",
            "or rnns ( rumelhart\n",
            "et al . ,\n",
            "1986a ) , are\n",
            "a family of neural\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe6o55hyWnyX"
      },
      "source": [
        " The `map` method allows to use any function to preprocess the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mFJ4jNloJk7"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbhGQ5k_XE5f"
      },
      "source": [
        "The model will predict input_text -> target_text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6erRolZ-omdr",
        "outputId": "d9eb8236-7a97-4093-f5e0-6ae63a968941",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', to_text(input_example.numpy()))\n",
        "  print ('Target data:', to_text(target_example.numpy()))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  recurrent neural networks\n",
            "Target data: neural networks ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74eDk9K_XXQ0"
      },
      "source": [
        "Finally we will shuffle the items, and produce minibatches of 16 elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcDexYkspBa9",
        "outputId": "d5304cd4-6d03-4e1f-f6c7-bb3b36975c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((16, 3), (16, 3)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX7Fdz3PXzxY"
      },
      "source": [
        "Let's test not trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNa3Ysj3qOEh",
        "outputId": "79d7f746-8dfe-4045-fc3b-7aa40b0d63b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 3, 493) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV13OaTQqcVU",
        "outputId": "be3a14e2-113e-451a-ced2-0803ba3dc77c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('input: ', to_text(input_example_batch.numpy()[0]))\n",
        "print('output:', to_text(target_example_batch.numpy()[0]))\n",
        "print('pred:  ', to_text(example_batch_predictions.numpy()[0].argmax(axis=1)))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  ) . the\n",
            "output: . the convolution\n",
            "pred:   several several several\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aqxQ_X-tsrV"
      },
      "source": [
        "## 5. Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7wT782Hrq-R",
        "outputId": "85935c6d-599e-4ba0-e2a7-af3e8d091268",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#model.set_weights(W0)\n",
        "history = model.fit(dataset, epochs=200, verbose=1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "33/33 [==============================] - 4s 7ms/step - loss: 5.7348 - accuracy: 0.0467\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 5.2107 - accuracy: 0.0518\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 5.0339 - accuracy: 0.0518\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.8670 - accuracy: 0.0707\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.7497 - accuracy: 0.0726\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.6632 - accuracy: 0.0789\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 4.5481 - accuracy: 0.1023\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.4698 - accuracy: 0.1143\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.3606 - accuracy: 0.1199\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.2900 - accuracy: 0.1351\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.2016 - accuracy: 0.1338\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.1227 - accuracy: 0.1521\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4.0586 - accuracy: 0.1427\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.9760 - accuracy: 0.1572\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.9201 - accuracy: 0.1717\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.8479 - accuracy: 0.1787\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.7911 - accuracy: 0.1907\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.7186 - accuracy: 0.2014\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 3.6665 - accuracy: 0.2115\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.5995 - accuracy: 0.2229\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.5383 - accuracy: 0.2285\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.4942 - accuracy: 0.2393\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.4449 - accuracy: 0.2399\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.3875 - accuracy: 0.2544\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.3335 - accuracy: 0.2576\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.2969 - accuracy: 0.2614\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.2292 - accuracy: 0.2790\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.1979 - accuracy: 0.2784\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 3.1423 - accuracy: 0.2917\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 3.1073 - accuracy: 0.2904\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 3.0516 - accuracy: 0.3049\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 3.0053 - accuracy: 0.3131\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.9747 - accuracy: 0.3213\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.9207 - accuracy: 0.3251\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.8824 - accuracy: 0.3333\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.8386 - accuracy: 0.3466\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.7864 - accuracy: 0.3548\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.7585 - accuracy: 0.3580\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.7198 - accuracy: 0.3674\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.6794 - accuracy: 0.3769\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.6261 - accuracy: 0.3883\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.5916 - accuracy: 0.3927\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.5471 - accuracy: 0.4160\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.5067 - accuracy: 0.4255\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.4803 - accuracy: 0.4154\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.4218 - accuracy: 0.4463\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.3982 - accuracy: 0.4394\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.3604 - accuracy: 0.4495\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.3199 - accuracy: 0.4672\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.2794 - accuracy: 0.4665\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.2365 - accuracy: 0.4975\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.2009 - accuracy: 0.4956\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1551 - accuracy: 0.5107\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.1249 - accuracy: 0.5152\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.0910 - accuracy: 0.5246\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 2.0481 - accuracy: 0.5398\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 2.0336 - accuracy: 0.5341\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.9828 - accuracy: 0.5549\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.9641 - accuracy: 0.5455\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.9196 - accuracy: 0.5694\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8824 - accuracy: 0.5764\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.8618 - accuracy: 0.5840\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.8351 - accuracy: 0.5878\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7913 - accuracy: 0.5941\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7655 - accuracy: 0.6105\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.7341 - accuracy: 0.6105\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.7137 - accuracy: 0.6010\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.6619 - accuracy: 0.6294\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.6471 - accuracy: 0.6301\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.6129 - accuracy: 0.6288\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5890 - accuracy: 0.6364\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5666 - accuracy: 0.6471\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5323 - accuracy: 0.6477\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.5246 - accuracy: 0.6496\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4811 - accuracy: 0.6566\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4585 - accuracy: 0.6629\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4395 - accuracy: 0.6654\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.4140 - accuracy: 0.6761\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3912 - accuracy: 0.6774\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.3633 - accuracy: 0.6717\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3392 - accuracy: 0.6755\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.3358 - accuracy: 0.6787\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2903 - accuracy: 0.7008\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2872 - accuracy: 0.6894\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2583 - accuracy: 0.7001\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2509 - accuracy: 0.6995\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.2322 - accuracy: 0.6951\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.2041 - accuracy: 0.7128\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.1912 - accuracy: 0.7146\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1772 - accuracy: 0.7109\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1442 - accuracy: 0.7216\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1397 - accuracy: 0.7184\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1236 - accuracy: 0.7191\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.1090 - accuracy: 0.7216\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0827 - accuracy: 0.7266\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0803 - accuracy: 0.7285\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0577 - accuracy: 0.7367\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0412 - accuracy: 0.7374\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0283 - accuracy: 0.7399\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 1.0246 - accuracy: 0.7399\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 1.0031 - accuracy: 0.7449\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9862 - accuracy: 0.7532\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9730 - accuracy: 0.7431\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9701 - accuracy: 0.7462\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9504 - accuracy: 0.7443\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9495 - accuracy: 0.7443\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9336 - accuracy: 0.7449\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9180 - accuracy: 0.7487\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.9119 - accuracy: 0.7538\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8990 - accuracy: 0.7557\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.9056 - accuracy: 0.7538\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.8857 - accuracy: 0.7538\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8760 - accuracy: 0.7551\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8687 - accuracy: 0.7601\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8571 - accuracy: 0.7595\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8568 - accuracy: 0.7607\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8458 - accuracy: 0.7601\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8376 - accuracy: 0.7601\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8262 - accuracy: 0.7626\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8283 - accuracy: 0.7582\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8220 - accuracy: 0.7620\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.8176 - accuracy: 0.7601\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 0.8052 - accuracy: 0.7639\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 0.7973 - accuracy: 0.7652\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.7903 - accuracy: 0.7708\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 0.7858 - accuracy: 0.7670\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 11ms/step - loss: 0.7769 - accuracy: 0.7715\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7793 - accuracy: 0.7658\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 13ms/step - loss: 0.7687 - accuracy: 0.7620\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 10ms/step - loss: 0.7592 - accuracy: 0.7677\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7591 - accuracy: 0.7652\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.7550 - accuracy: 0.7702\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.7475 - accuracy: 0.7696\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7412 - accuracy: 0.7683\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7469 - accuracy: 0.7740\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7355 - accuracy: 0.7689\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7303 - accuracy: 0.7715\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7287 - accuracy: 0.7746\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7245 - accuracy: 0.7614\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 0.7203 - accuracy: 0.7715\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7154 - accuracy: 0.7740\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.7090 - accuracy: 0.7708\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7098 - accuracy: 0.7740\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7074 - accuracy: 0.7727\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7035 - accuracy: 0.7689\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6981 - accuracy: 0.7727\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.7001 - accuracy: 0.7765\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6908 - accuracy: 0.7746\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6877 - accuracy: 0.7645\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6814 - accuracy: 0.7727\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6805 - accuracy: 0.7708\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6764 - accuracy: 0.7702\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6865 - accuracy: 0.7708\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6733 - accuracy: 0.7677\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6694 - accuracy: 0.7746\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6680 - accuracy: 0.7753\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6621 - accuracy: 0.7753\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6596 - accuracy: 0.7753\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6688 - accuracy: 0.7740\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6589 - accuracy: 0.7708\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6528 - accuracy: 0.7702\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6680 - accuracy: 0.7683\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6448 - accuracy: 0.7740\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6477 - accuracy: 0.7790\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6460 - accuracy: 0.7721\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6567 - accuracy: 0.7696\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6397 - accuracy: 0.7740\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6379 - accuracy: 0.7778\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6384 - accuracy: 0.7683\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6445 - accuracy: 0.7740\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6411 - accuracy: 0.7683\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6333 - accuracy: 0.7715\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6329 - accuracy: 0.7708\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6397 - accuracy: 0.7702\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6269 - accuracy: 0.7753\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6219 - accuracy: 0.7727\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6261 - accuracy: 0.7715\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6263 - accuracy: 0.7715\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6233 - accuracy: 0.7683\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6174 - accuracy: 0.7746\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6213 - accuracy: 0.7734\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6127 - accuracy: 0.7664\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6169 - accuracy: 0.7670\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6190 - accuracy: 0.7753\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6099 - accuracy: 0.7746\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6104 - accuracy: 0.7721\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6118 - accuracy: 0.7696\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6048 - accuracy: 0.7721\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6045 - accuracy: 0.7727\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6039 - accuracy: 0.7677\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6059 - accuracy: 0.7753\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6134 - accuracy: 0.7670\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6067 - accuracy: 0.7753\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6018 - accuracy: 0.7759\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.6009 - accuracy: 0.7740\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 0.5992 - accuracy: 0.7708\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.5972 - accuracy: 0.7689\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.6000 - accuracy: 0.7727\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.5954 - accuracy: 0.7734\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 0.5949 - accuracy: 0.7702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dan-chKsvbU"
      },
      "source": [
        "def draw_history(hist):\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
        "  axs[0].plot(hist.epoch, hist.history['loss'])\n",
        "  if 'val_loss' in hist.history:\n",
        "    axs[0].plot(hist.epoch, hist.history['val_loss'])\n",
        "  axs[0].legend(('training loss', 'validation loss'))\n",
        "  axs[1].plot(hist.epoch, hist.history['accuracy'])\n",
        "  if 'val_accuracy' in hist.history:\n",
        "    axs[1].plot(hist.epoch, hist.history['val_accuracy'])\n",
        "\n",
        "  axs[1].legend(('training accuracy', 'validation accuracy'))\n",
        "  plt.show()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAJu4rMys3da",
        "outputId": "cb8e1552-3814-4bcc-bfca-80b3a2924fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "draw_history(history)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BldvImFYfm4"
      },
      "source": [
        "## 6. Generating text with RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3kuXO1jYu-a"
      },
      "source": [
        "Take word sequence and generate the following 128 words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyTL_hrbxwj-"
      },
      "source": [
        "def gen_long(model, word_id_arr, n_words=128):\n",
        "  out = []\n",
        "  words = list(word_id_arr.copy())\n",
        "  for i in range(n_words):\n",
        "      keys = np.reshape(np.array(words), [-1, n_input])\n",
        "\n",
        "      onehot_pred = model(keys).numpy()[0]\n",
        "      pred_index = onehot_pred.argmax(axis=1)\n",
        "      pred = pred_index[-1]\n",
        "      out.append(pred)\n",
        "\n",
        "      words = words[1:]\n",
        "      words.append(pred)\n",
        "  sentence = to_text(out)\n",
        "  return sentence"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7gzaQ1FyRCE",
        "outputId": "4fc4e307-07b6-413e-ad77-58aa948ecbb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(10):\n",
        "  input_seq = input_example_batch.numpy()[0]\n",
        "  sentence = gen_long(model, input_seq)\n",
        "  print(to_text(input_seq), '...')\n",
        "  print('\\t...', sentence, '\\n')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn is used ...\n",
            "\t... in equation , . . . x . 1 ) = x ( t ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , \n",
            "\n",
            "of the previous ...\n",
            "\t... members function , t t ; theta ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) \n",
            "\n",
            "with a function ...\n",
            "\t... g to the go f . , . . . x . 1 ) = x ( t ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x \n",
            "\n",
            "with more precision ...\n",
            "\t... than loss the different f . for we expression g ( t ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) \n",
            "\n",
            ". 1 for ...\n",
            "\t... a whole on network steps such example might statistical the it in a function g to the go f . , . . . x . 1 ) = x ( t ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) \n",
            "\n",
            "x ( t ...\n",
            "\t... ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta \n",
            "\n",
            "time step , ...\n",
            "\t... representing the classical g to make ( the the sentence with sufficient all each at model . . x . 1 ) = x ( t ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; \n",
            "\n",
            ", h ( ...\n",
            "\t... t ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; \n",
            "\n",
            "convolution kernel at ...\n",
            "\t... each time steps the convolution the unfolded side of parameters of - time t , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; \n",
            "\n",
            "nor share statistical ...\n",
            "\t... strength it is small by the external that unfolded word applying be black in it with at forms , we system rewrite , x ( t ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGv_d3bzZB9a"
      },
      "source": [
        "Or try to input some text and see continuation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "_iX7hcrFtsrW",
        "outputId": "2e558b14-2804-49ad-c5d7-a5b6f04c261a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "while True:\n",
        "    prompt = \"%s words: \" % n_input\n",
        "\n",
        "    try:\n",
        "      sentence = input(prompt)\n",
        "    except KeyboardInterrupt:\n",
        "      break\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    words = sentence.split(' ')\n",
        "    if len(words) != n_input:\n",
        "        continue\n",
        "    try:\n",
        "        symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
        "    except:\n",
        "        print(\"Word not in dictionary\")\n",
        "        continue\n",
        "\n",
        "    sentence = gen_long(model, symbols_in_keys)\n",
        "    print(sentence)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 words: The network is\n",
            "Word not in dictionary\n",
            "3 words: the network is\n",
            "the neural network is equation 10 . 4 or 10 ( t ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x ) ; theta , x )\n",
            "3 words: iteration text run\n",
            "Word not in dictionary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCKy-0EotsrX"
      },
      "source": [
        "## 7. Exercise \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYXY5jfMAi6x"
      },
      "source": [
        "* Run with 5-7 input words instead of 3.\n",
        "* increase number of training iterations, since convergance will take much longer (training as well!)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "n_input = 8  # word sequence to predict the following word\n",
        "\n",
        "# number of units in RNN cells\n",
        "n_hidden = [256, 512, 128]\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 128, input_length=n_input))\n",
        "\n",
        "for layer_i, n_h in enumerate(n_hidden):\n",
        "  model.add(tf.keras.layers.LSTM(n_h, return_sequences=True, name=f'{layer_i}_lstm{n_h}'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='RMSProp',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "W0 = model.get_weights()  # to reset model to original state:\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "g5qMMQZfQS_J",
        "outputId": "28b711b4-2cc9-45fd-af03-997ddae030f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 8, 128)            63104     \n",
            "                                                                 \n",
            " 0_lstm256 (LSTM)            (None, 8, 256)            394240    \n",
            "                                                                 \n",
            " 1_lstm512 (LSTM)            (None, 8, 512)            1574912   \n",
            "                                                                 \n",
            " 2_lstm128 (LSTM)            (None, 8, 128)            328192    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8, 493)            63597     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,424,045\n",
            "Trainable params: 2,424,045\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "id": "ySBawX1VQwbF",
        "outputId": "afbe193b-4c2e-4a44-c656-b31a6e8cface",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((16, 16, 3), (16, 16, 3)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.set_weights(W0)\n",
        "history = model.fit(dataset, epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "CEHvtzVUQ32-",
        "outputId": "6038a6d3-6f79-48fb-c651-40a81bd33e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b8551a95f26a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model.set_weights(W0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 8), found shape=(16, 16, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7-p8ClctsrY"
      },
      "source": [
        "## 8. Further reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjhwEgdKtsrZ"
      },
      "source": [
        "[Illustrated Guide to Recurrent Neural Networks](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
        "\n",
        "[Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)"
      ]
    }
  ]
}